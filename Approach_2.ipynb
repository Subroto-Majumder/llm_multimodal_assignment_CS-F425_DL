{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32261f4f",
   "metadata": {},
   "source": [
    "# Approach 2: LLM + Adapters (Frozen Models + Trainable Connector)\n",
    "\n",
    "This notebook demonstrates a **multimodal adapter** approach. Unlike the pipeline approach (where models are separate), here we **fuse** vision and language by training a small \"connector\" layer.\n",
    "\n",
    "**Key Concept:**\n",
    "We take a powerful pre-trained Vision Encoder (CLIP) and a powerful pre-trained LLM (Phi-3). We keep both of them **FROZEN** (unchanged). We only train a tiny **Adapter** (MLP) that learns to translate \"visual features\" into \"word embeddings\" that the LLM can understand.\n",
    "\n",
    "**Architecture:**\n",
    "1.  **Image** $\\rightarrow$ Frozen CLIP $\\rightarrow$ Visual Features\n",
    "2.  **Visual Features** $\\rightarrow$ **Trainable Adapter** $\\rightarrow$ Visual Tokens\n",
    "3.  **Visual Tokens** + **Text Tokens** $\\rightarrow$ Frozen LLM $\\rightarrow$ Text Output\n",
    "4.  **LLM** $\\rightarrow$ Image Prompt $\\rightarrow$ Frozen Stable Diffusion $\\rightarrow$ Output Image\n",
    "\n",
    "**Why this matters:**\n",
    "This allows the LLM to \"see\" the image directly in its embedding space, enabling joint reasoning (e.g., \"What is unusual about this image?\") without retraining the massive LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# - peft: Parameter-Efficient Fine-Tuning (for LoRA/Adapters)\n",
    "# - bitsandbytes: For 4-bit quantization\n",
    "# - datasets: To easily download COCO subset\n",
    "!pip install -q transformers diffusers accelerate bitsandbytes peft torch torchvision datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfff985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def show_image(img, title=None):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img)\n",
    "    if title: plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d04885",
   "metadata": {},
   "source": [
    "## 1. Load Frozen Vision Encoder (CLIP)\n",
    "We use `openai/clip-vit-base-patch32`. We set `requires_grad=False` to ensure it is **frozen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "print(\"Loading CLIP Vision Encoder...\")\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_processor = CLIPImageProcessor.from_pretrained(clip_model_name)\n",
    "clip_model = CLIPVisionModel.from_pretrained(clip_model_name).to(device)\n",
    "\n",
    "# FREEZE the vision encoder\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"CLIP loaded and FROZEN.\")\n",
    "print(f\"Vision Output Dimension: {clip_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd1c1a7",
   "metadata": {},
   "source": [
    "## 2. Define Trainable Adapter (The \"Connector\")\n",
    "This is the **only** part we will train from scratch. It projects the 768-dim CLIP features to the 3072-dim Phi-3 embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf95ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualAdapter(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(output_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Dimensions\n",
    "CLIP_DIM = 768 # ViT-B/32 hidden size\n",
    "PHI_DIM = 3072 # Phi-3-mini hidden size\n",
    "\n",
    "# Initialize Adapter\n",
    "adapter = VisualAdapter(CLIP_DIM, PHI_DIM).to(device)\n",
    "print(\"Adapter initialized (Trainable).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e5175",
   "metadata": {},
   "source": [
    "## 3. Load Frozen LLM (Phi-3-mini)\n",
    "We load Phi-3 in 4-bit to save memory. We also freeze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dfb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"Loading Phi-3 LLM...\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False # Use internal implementation\n",
    ")\n",
    "\n",
    "# FREEZE the LLM\n",
    "for param in llm_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Phi-3 loaded and FROZEN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56209ef5",
   "metadata": {},
   "source": [
    "## 4. Tiny Training Loop (Demonstration)\n",
    "We will train the adapter on a small example dataset(random subset of COCO captions). Goal to teachc the model that an image corresponds to its text caption\n",
    "\n",
    "**Process:**\n",
    "1.  Get Image Embeddings (CLIP).\n",
    "2.  Project them (Adapter).\n",
    "3.  Concatenate with Text Embeddings (LLM).\n",
    "4.  Calculate Loss.\n",
    "5.  Update Adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2116e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load Dataset (COCO Subset)\n",
    "print(\"Loading COCO dataset subset...\")\n",
    "try:\n",
    "    # We use 'phiyodr/coco2017' which contains metadata/captions.\n",
    "    dataset = load_dataset(\"phiyodr/coco2017\", split=\"train[:50]\")\n",
    "    print(f\"Loaded {len(dataset)} examples from COCO.\")\n",
    "except Exception as e:\n",
    "    print(f\"COCO load failed: {e}. Falling back to Pokemon dataset.\")\n",
    "    dataset = load_dataset(\"lambdalabs/pokemon-blip-captions\", split=\"train[:50]\")\n",
    "\n",
    "# 2. Setup Optimizer\n",
    "optimizer = torch.optim.AdamW(adapter.parameters(), lr=1e-4)\n",
    "adapter.train()\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "# Headers to mimic a browser to avoid 403 Forbidden on some servers\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "for idx, item in enumerate(dataset):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # --- Prepare Data ---\n",
    "    raw_image = None\n",
    "    caption = \"\"\n",
    "    \n",
    "    # Case 1: Dataset has 'image' column (PIL Image) - e.g. Pokemon or full COCO\n",
    "    if 'image' in item and item['image'] is not None:\n",
    "        raw_image = item['image'].convert('RGB')\n",
    "    \n",
    "    # Case 2: Dataset has 'file_name' (COCO style), fetch from URL\n",
    "    elif 'file_name' in item:\n",
    "        try:\n",
    "            # Try train2017 first\n",
    "            file_name = item['file_name']\n",
    "            img_url = f\"http://images.cocodataset.org/train2017/{file_name}\"\n",
    "            \n",
    "            # Use BytesIO(response.content) which is more robust than response.raw\n",
    "            response = requests.get(img_url, headers=headers, timeout=5)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                # Try val2017 fallback\n",
    "                img_url = f\"http://images.cocodataset.org/val2017/{file_name}\"\n",
    "                response = requests.get(img_url, headers=headers, timeout=5)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                raw_image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            else:\n",
    "                # print(f\"Failed to fetch {file_name}: {response.status_code}\")\n",
    "                pass\n",
    "                \n",
    "        except Exception as e:\n",
    "            # print(f\"Error downloading {item.get('file_name')}: {e}\")\n",
    "            pass\n",
    "            \n",
    "    if raw_image is None:\n",
    "        continue\n",
    "\n",
    "    # Get Caption\n",
    "    if 'captions' in item:\n",
    "        caption = item['captions'][0] # COCO\n",
    "    elif 'text' in item:\n",
    "        caption = item['text'] # Pokemon/BLIP datasets\n",
    "    else:\n",
    "        caption = \"An image.\"\n",
    "        \n",
    "    instruction = \"Describe this image: \"\n",
    "    target_text = caption\n",
    "    \n",
    "    # --- A. Vision Forward ---\n",
    "    with torch.no_grad():\n",
    "        pixel_values = clip_processor(images=raw_image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        vision_outputs = clip_model(pixel_values)\n",
    "        image_embeds_raw = vision_outputs.pooler_output\n",
    "        \n",
    "    # Pass through Adapter\n",
    "    image_embeds_projected = adapter(image_embeds_raw.unsqueeze(1))\n",
    "    \n",
    "    # --- B. Text Forward ---\n",
    "    text_input = instruction + target_text\n",
    "    tokens = llm_tokenizer(text_input, return_tensors=\"pt\").to(device)\n",
    "    input_ids = tokens.input_ids\n",
    "    \n",
    "    instruction_len = len(llm_tokenizer(instruction).input_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_embeds = llm_model.model.embed_tokens(input_ids)\n",
    "        \n",
    "    # --- C. Combine & Loss ---\n",
    "    # Cast image embeddings to match text embeddings dtype (float16)\n",
    "    image_embeds_projected = image_embeds_projected.to(text_embeds.dtype)\n",
    "    \n",
    "    inputs_embeds = torch.cat([image_embeds_projected, text_embeds], dim=1)\n",
    "    \n",
    "    # Create Labels\n",
    "    labels = torch.full(inputs_embeds.shape[:2], -100, dtype=torch.long).to(device)\n",
    "    start_idx = 1 + instruction_len\n",
    "    if input_ids.shape[1] > instruction_len:\n",
    "        labels[0, start_idx:] = input_ids[0, instruction_len:]\n",
    "    \n",
    "    # LLM Forward\n",
    "    outputs = llm_model(inputs_embeds=inputs_embeds, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Step {idx+1}/{len(dataset)}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete. Adapter updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234dd3c2",
   "metadata": {},
   "source": [
    "## 5. Inference Demo\n",
    "Now we use the trained adapter to perform inference.\n",
    "1. **Image** $\\rightarrow$ Adapter $\\rightarrow$ LLM\n",
    "2. **Text** $\\rightarrow$ LLM\n",
    "3. **LLM** generates response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multimodal(image, prompt, max_new_tokens=50):\n",
    "    adapter.eval()\n",
    "    \n",
    "    # 1. Vision Path\n",
    "    with torch.no_grad():\n",
    "        pixel_values = clip_processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        vision_outputs = clip_model(pixel_values)\n",
    "        image_embeds = adapter(vision_outputs.pooler_output.unsqueeze(1))\n",
    "        \n",
    "    # 2. Text Path\n",
    "    tokens = llm_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = tokens.input_ids\n",
    "    with torch.no_grad():\n",
    "        text_embeds = llm_model.model.embed_tokens(input_ids)\n",
    "        \n",
    "    # 3. Combine\n",
    "    # CRITICAL FIX: Cast image embeddings to match text embeddings dtype (float16)\n",
    "    image_embeds = image_embeds.to(text_embeds.dtype)\n",
    "    inputs_embeds = torch.cat([image_embeds, text_embeds], dim=1)\n",
    "    \n",
    "    # 4. Generate\n",
    "    attention_mask = torch.ones(inputs_embeds.shape[:2], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            pad_token_id=llm_tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "    return llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test it\n",
    "# We load a fresh image to ensure the variable is valid and avoid \"dtype object\" errors\n",
    "print(\"Loading test image for inference...\")\n",
    "img_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\" # Two cats\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "try:\n",
    "    response = requests.get(img_url, headers=headers, timeout=5)\n",
    "    test_image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load test image ({e}). Using dummy image.\")\n",
    "    test_image = Image.new('RGB', (224, 224), color='gray')\n",
    "\n",
    "test_prompt = \"Describe this image and suggest a style for a painting of it.\"\n",
    "print(f\"Input Prompt: {test_prompt}\")\n",
    "show_image(test_image, \"Input\")\n",
    "\n",
    "response = generate_multimodal(test_image, test_prompt)\n",
    "print(f\"LLM Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7edd0",
   "metadata": {},
   "source": [
    "## 6. Generate Image (Frozen Stable Diffusion)\n",
    "Finally, we take the LLM's suggestion and generate an image.\n",
    "(Note: Since we only trained on a small example dataset, the LLM's output might be random, but the pipeline is functional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9201505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load Stable Diffusion (Frozen)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "pipe.to(device)\n",
    "\n",
    "# Extract a prompt from the response (or use a fallback if response is gibberish due to tiny training)\n",
    "# For this demo, we'll assume the response contains a description.\n",
    "gen_prompt = response if len(response) > 5 else \"A cat in cyberpunk style\"\n",
    "\n",
    "print(f\"Generating image for: '{gen_prompt}'\")\n",
    "image = pipe(gen_prompt).images[0]\n",
    "show_image(image, \"Generated Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee616b",
   "metadata": {},
   "source": [
    "# Findings\n",
    "Performs very poorly, and does not match the original image, this is mainly due to a very small and not carefully chosend dataset. Production grade ones like LLaVA train on thousands of images, this was not done here due to computational constraints"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
