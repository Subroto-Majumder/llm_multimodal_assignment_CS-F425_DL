{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7556c88",
   "metadata": {},
   "source": [
    "# Approach 1: LLM + Tools (Pipeline Multimodal AI)\n",
    "\n",
    "This notebook demonstrates a **pipeline** approach to building a multimodal AI system. Instead of a single massive model that does everything, we chain together specialized models:\n",
    "\n",
    "1.  **Speech-to-Text (Whisper):** Converts input audio into text (transcription).\n",
    "2.  **Reasoning Engine (LLM - Phi-3):** Takes the transcription, reasons about it, and generates a text response AND a prompt for a new image.\n",
    "3.  **Image Generator (Stable Diffusion):** Takes the prompt from the LLM and generates a new image.\n",
    "\n",
    "**Architecture:**\n",
    "`Audio` → `[Speech-to-Text]` → `Text Transcription`\n",
    "`Text Transcription` → `[LLM]` → `Text Response` + `Image Prompt`\n",
    "`Image Prompt` → `[Image Generator]` → `Output Image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# - transformers: For Phi-3\n",
    "# - diffusers: For Stable Diffusion\n",
    "# - accelerate: For model offloading and optimization\n",
    "# - bitsandbytes: For 4-bit quantization of the LLM\n",
    "# - openai-whisper: For Speech-to-Text\n",
    "!pip install -q transformers diffusers accelerate bitsandbytes torch torchvision openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50570d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import whisper\n",
    "\n",
    "# Setup device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Helper to display images\n",
    "def show_image(img, title=None):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5cbc44",
   "metadata": {},
   "source": [
    "## 1. Load Speech-to-Text Model (Whisper)\n",
    "We use `openai/whisper` (base model). It's fast and accurate for general English transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Whisper model...\")\n",
    "# Load the 'base' model. Options: tiny, base, small, medium, large\n",
    "whisper_model = whisper.load_model(\"base\", device=device)\n",
    "print(\"Whisper loaded.\")\n",
    "\n",
    "def audio_to_text(audio_path):\n",
    "    result = whisper_model.transcribe(audio_path)\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69c78b7",
   "metadata": {},
   "source": [
    "## 2. Load Reasoning LLM (Phi-3-mini)\n",
    "We use `microsoft/Phi-3-mini-4k-instruct`. To ensure it fits in 8GB VRAM alongside other models, we load it in **4-bit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"Loading Phi-3 LLM...\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\", \n",
    "    trust_remote_code=False # Use internal Transformers implementation to avoid DynamicCache errors\n",
    ")\n",
    "print(\"Phi-3 loaded.\")\n",
    "\n",
    "def run_llm(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    input_ids = llm_tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n",
    "    \n",
    "    # Explicitly set attention mask and pad_token_id to avoid warnings/errors\n",
    "    attention_mask = (input_ids != llm_tokenizer.pad_token_id).long() if llm_tokenizer.pad_token_id is not None else torch.ones_like(input_ids)\n",
    "    \n",
    "    outputs = llm_model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256, \n",
    "        do_sample=True, \n",
    "        temperature=0.7,\n",
    "        pad_token_id=llm_tokenizer.eos_token_id\n",
    "    )\n",
    "    text = llm_tokenizer.batch_decode(outputs[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53d3a3",
   "metadata": {},
   "source": [
    "## 3. Load Image Generation Model (Stable Diffusion 1.5)\n",
    "We use `runwayml/stable-diffusion-v1-5`. We load it in `float16` to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "print(\"Loading Stable Diffusion...\")\n",
    "# We use enable_model_cpu_offload() to save VRAM when the model is not in use\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", \n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipe.to(device)\n",
    "# pipe.enable_model_cpu_offload() # Optional: Enable if you run out of VRAM (requires 'accelerate')\n",
    "print(\"Stable Diffusion loaded.\")\n",
    "\n",
    "def text_to_image(prompt):\n",
    "    image = pipe(prompt).images[0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c525e",
   "metadata": {},
   "source": [
    "## 4. End-to-End Pipeline Demo\n",
    "Now we combine everything.\n",
    "1. **Input:** An audio file.\n",
    "2. **Step 1:** Whisper hears the audio -> transcription.\n",
    "3. **Step 2:** LLM sees transcription -> Generates a text reply AND a new image prompt.\n",
    "4. **Step 3:** Stable Diffusion generates the new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33fcd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. INPUT ---\n",
    "# Load a sample audio file\n",
    "# Ensure you have an audio file named 'example_audio.mp3' or similar in your directory\n",
    "audio_file = \"example_audio.mp3\" \n",
    "\n",
    "# If you don't have a file, uncomment the following lines to download a sample (if available)\n",
    "# !wget -O example_audio.mp3 https://www2.cs.uic.edu/~i101/SoundFiles/BabyElephantWalk60.wav\n",
    "\n",
    "print(f\"Processing Audio File: {audio_file}\")\n",
    "\n",
    "# --- 2. HEAR (Audio to Text) ---\n",
    "try:\n",
    "    transcription = audio_to_text(audio_file)\n",
    "    print(f\"\\n[Whisper] Transcription: {transcription}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing audio: {e}\")\n",
    "    transcription = \"A futuristic city with flying cars and neon lights.\" # Fallback for demo purposes\n",
    "\n",
    "# --- 3. THINK (LLM Reasoning) ---\n",
    "# We construct a structured prompt for the LLM\n",
    "llm_prompt = f\"\"\"\n",
    "You are a creative AI assistant.\n",
    "I will provide a transcription of an audio recording.\n",
    "Your job is to:\n",
    "1. Improve and refine the description from the audio.\n",
    "2. Create a concise, high-quality image generation prompt based on the refined description.\n",
    "\n",
    "Audio Transcription: {transcription}\n",
    "\n",
    "Format your response EXACTLY like this:\n",
    "RESPONSE: [Your refined description here]\n",
    "IMAGE_PROMPT: [Your image generation prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n[LLM] Thinking...\")\n",
    "llm_output = run_llm(llm_prompt)\n",
    "print(f\"[LLM] Raw Output:\\n{llm_output}\\n\")\n",
    "\n",
    "# Parse the output (Simple string parsing)\n",
    "try:\n",
    "    response_text = llm_output.split(\"RESPONSE:\")[1].split(\"IMAGE_PROMPT:\")[0].strip()\n",
    "    image_prompt = llm_output.split(\"IMAGE_PROMPT:\")[1].strip()\n",
    "except IndexError:\n",
    "    print(\"Error parsing LLM output. Using raw output as prompt.\")\n",
    "    response_text = llm_output\n",
    "    image_prompt = transcription # Fallback\n",
    "\n",
    "print(f\"Parsed Response: {response_text}\")\n",
    "print(f\"Parsed Image Prompt: {image_prompt}\")\n",
    "\n",
    "# --- 4. CREATE (Text to Image) ---\n",
    "print(f\"\\n[Stable Diffusion] Generating image for: '{image_prompt}'...\")\n",
    "generated_image = text_to_image(image_prompt)\n",
    "\n",
    "show_image(generated_image, \"Generated Output\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
